# Week 3 Day 5 Hands-on 1: ê³ ê¸‰ ìš´ì˜ ê¸°ëŠ¥

<div align="center">

**ğŸ¯ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­** â€¢ **ğŸ”” ê³ ê¸‰ ì•Œë¦¼** â€¢ **ğŸŒ ë©€í‹° í´ëŸ¬ìŠ¤í„°** â€¢ **ğŸ“¦ Helm ê³ ê¸‰**

*Lab 1ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œë•ì…˜ê¸‰ ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„*

</div>

---

## âš ï¸ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### ë¹ ë¥¸ ì‹œì‘ (ìë™ í™˜ê²½ ì„¤ì •)
```bash
cd lab_scripts/handson1
./00-setup-environment.sh
```

**ìë™ ì„¤ì¹˜ í•­ëª©**:
- âœ… Kubernetes í´ëŸ¬ìŠ¤í„° (challenge-cluster, ì—†ìœ¼ë©´ ìë™ ìƒì„±)
- âœ… day5-handson Namespace
- âœ… Helm
- âœ… Prometheus Operator (ServiceMonitor CRD í¬í•¨)
- âœ… Metrics Server

### ìˆ˜ë™ í™˜ê²½ í™•ì¸
```bash
# í´ëŸ¬ìŠ¤í„° í™•ì¸
kubectl cluster-info

# Prometheus Operator CRD í™•ì¸
kubectl get crd servicemonitors.monitoring.coreos.com

# Namespace í™•ì¸
kubectl get namespace day5-handson monitoring
```

---

## ğŸ•˜ ì‹¤ìŠµ ì •ë³´
**ì‹œê°„**: 14:00-15:30 (90ë¶„)  
**ëª©í‘œ**: Lab 1 í™•ì¥ + ì‹¤ë¬´ ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„  
**ë°©ì‹**: Lab 1 ê¸°ë°˜ + ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€

## ğŸ¯ ì‹¤ìŠµ ëª©í‘œ

### ğŸ“š í•™ìŠµ ëª©í‘œ
- **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­**: Prometheus Adapterë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
- **ê³ ê¸‰ ì•Œë¦¼**: ë³µì¡í•œ ì•Œë¦¼ ê·œì¹™ê³¼ ë¼ìš°íŒ…
- **ë©€í‹° í´ëŸ¬ìŠ¤í„°**: ArgoCDë¡œ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬
- **Helm ê³ ê¸‰**: Chart ê°œë°œ ë° ë°°í¬ ìë™í™”

### ğŸ› ï¸ êµ¬í˜„ ëª©í‘œ
- HTTP ìš”ì²­ ìˆ˜ ê¸°ë°˜ HPA
- ê³„ì¸µì  ì•Œë¦¼ ì‹œìŠ¤í…œ (Slack, Email, PagerDuty)
- ë©€í‹° í´ëŸ¬ìŠ¤í„° GitOps êµ¬ì„±
- í”„ë¡œë•ì…˜ê¸‰ Helm Chart ì‘ì„±

---

## ğŸ—ï¸ ê³ ê¸‰ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸µ"
        A1[Web App<br/>+ Metrics Exporter]
        A2[Custom Metrics<br/>HPA]
    end
    
    subgraph "ëª¨ë‹ˆí„°ë§ ê³„ì¸µ"
        B1[Prometheus]
        B2[Prometheus Adapter]
        B3[Grafana]
        B4[AlertManager]
    end
    
    subgraph "ì•Œë¦¼ ê³„ì¸µ"
        C1[Slack]
        C2[Email]
        C3[PagerDuty]
    end
    
    subgraph "GitOps ê³„ì¸µ"
        D1[Git Repo]
        D2[ArgoCD Hub]
        D3[Dev Cluster]
        D4[Prod Cluster]
    end
    
    A1 -->|ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­| B1
    B1 --> B2
    B2 -->|ë©”íŠ¸ë¦­| A2
    B1 --> B3
    B1 --> B4
    
    B4 -->|Critical| C3
    B4 -->|Warning| C1
    B4 -->|Info| C2
    
    D1 --> D2
    D2 --> D3
    D2 --> D4
    
    style A1 fill:#e8f5e8
    style A2 fill:#fff3e0
    style B1 fill:#e3f2fd
    style B2 fill:#f3e5f5
    style B3 fill:#e8f5e8
    style B4 fill:#ffebee
    style C1 fill:#fff3e0
    style C2 fill:#e3f2fd
    style C3 fill:#ffebee
    style D1 fill:#e8f5e8
    style D2 fill:#fff3e0
    style D3 fill:#e3f2fd
    style D4 fill:#ffebee
```

---

## ğŸ› ï¸ Step 0: í™˜ê²½ ì„¤ì • (10ë¶„)

### Step 0-1: í´ëŸ¬ìŠ¤í„° ìƒì„±

**í´ëŸ¬ìŠ¤í„° í™•ì¸ ë° ìƒì„±**:
```bash
# í´ëŸ¬ìŠ¤í„° í™•ì¸
kubectl cluster-info

# ì—†ìœ¼ë©´ kind í´ëŸ¬ìŠ¤í„° ìƒì„±
kind create cluster --name challenge-cluster --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
EOF
```

### Step 0-2: Namespace ìƒì„±

```bash
# day5-handson namespace ìƒì„±
kubectl create namespace day5-handson

# monitoring namespace ìƒì„±
kubectl create namespace monitoring

# ê¸°ë³¸ namespace ì„¤ì •
kubectl config set-context --current --namespace=day5-handson
```

### Step 0-3: í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜

**Helm ì„¤ì¹˜**:
```bash
# Helm ì„¤ì¹˜
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Repository ì¶”ê°€
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

**Prometheus Operator ì„¤ì¹˜**:
```bash
# Prometheus Operator ì„¤ì¹˜ (ServiceMonitor CRD í¬í•¨)
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
  --set grafana.enabled=false \
  --wait
```

**Metrics Server ì„¤ì¹˜**:
```bash
# Metrics Server ì„¤ì¹˜
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# ì¤€ë¹„ ëŒ€ê¸° (30ì´ˆ)
sleep 30
```

**ArgoCD ì„¤ì¹˜**:
```bash
# argocd namespace ìƒì„±
kubectl create namespace argocd

# ArgoCD ì„¤ì¹˜
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# ArgoCD CLI ì„¤ì¹˜
curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd
rm argocd-linux-amd64

# ArgoCD ì¤€ë¹„ ëŒ€ê¸°
kubectl wait --for=condition=Ready pods --all -n argocd --timeout=300s
```

### Step 0-4: í™˜ê²½ í™•ì¸

```bash
# í´ëŸ¬ìŠ¤í„° ì •ë³´
kubectl cluster-info

# CRD í™•ì¸
kubectl get crd servicemonitors.monitoring.coreos.com

# Namespace í™•ì¸
kubectl get namespace day5-handson monitoring argocd

# ArgoCD CLI í™•ì¸
argocd version --client

# í˜„ì¬ namespace í™•ì¸
kubectl config view --minify | grep namespace:
```

---

## ğŸ› ï¸ Step 1: ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê¸°ë°˜ HPA (25ë¶„)

### Step 1-1: ë©”íŠ¸ë¦­ì„ ë…¸ì¶œí•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬

```yaml
# metrics-app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-app
  namespace: day5-handson
spec:
  replicas: 2
  selector:
    matchLabels:
      app: metrics-app
  template:
    metadata:
      labels:
        app: metrics-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: quay.io/brancz/prometheus-example-app:v0.3.0
        ports:
        - containerPort: 8080
          name: metrics
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-app
  namespace: day5-handson
  labels:
    app: metrics-app
spec:
  selector:
    app: metrics-app
  ports:
  - port: 8080
    targetPort: 8080
    name: metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: metrics-app
  namespace: day5-handson
spec:
  selector:
    matchLabels:
      app: metrics-app
  endpoints:
  - port: metrics
    interval: 15s
```

```bash
# ë°°í¬
kubectl apply -f metrics-app-deployment.yaml -n day5-handson

# ë©”íŠ¸ë¦­ í™•ì¸
kubectl port-forward -n day5-handson svc/metrics-app 8080:8080
curl http://localhost:8080/metrics
```

### Step 1-2: Prometheus Adapter ì„¤ì¹˜

```bash
# Prometheus Adapter ì„¤ì¹˜
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  --set prometheus.url=http://prometheus-kube-prometheus-prometheus.monitoring.svc \
  --set prometheus.port=9090

# ì„¤ì¹˜ í™•ì¸
kubectl get pods -n monitoring | grep adapter

# Custom Metrics API í™•ì¸
kubectl get apiservice v1beta1.custom.metrics.k8s.io
```

### Step 1-3: ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì„¤ì •

```yaml
# prometheus-adapter-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter
  namespace: monitoring
data:
  config.yaml: |
    rules:
    # HTTP ìš”ì²­ ìˆ˜ ë©”íŠ¸ë¦­
    - seriesQuery: 'http_requests_total{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^(.*)_total$"
        as: "${1}_per_second"
      metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'
    
    # ì• í”Œë¦¬ì¼€ì´ì…˜ í ê¸¸ì´
    - seriesQuery: 'queue_length{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        as: "queue_length"
      metricsQuery: 'avg(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)'
```

```bash
# ConfigMap ì—…ë°ì´íŠ¸
kubectl apply -f prometheus-adapter-config.yaml

# Adapter ì¬ì‹œì‘
kubectl rollout restart deployment prometheus-adapter -n monitoring

# ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ í™•ì¸
kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1" | jq .
```

### Step 1-4: ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê¸°ë°˜ HPA ìƒì„±

```yaml
# custom-metrics-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: metrics-app-hpa
  namespace: day5-handson
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: metrics-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­: HTTP ìš”ì²­ ìˆ˜
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
```

```bash
# HPA ìƒì„±
kubectl apply -f custom-metrics-hpa.yaml -n day5-handson

# HPA ìƒíƒœ í™•ì¸
kubectl get hpa -n day5-handson metrics-app-hpa
kubectl describe hpa -n day5-handson metrics-app-hpa
```

### Step 1-5: ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# ë¶€í•˜ ìƒì„±
kubectl run -n day5-handson load-generator --image=busybox --restart=Never -- /bin/sh -c \
  "while true; do wget -q -O- http://metrics-app:8080; done"

# HPA ë™ì‘ ê´€ì°°
watch kubectl get hpa -n day5-handson metrics-app-hpa

# ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ í™•ì¸
kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/day5-handson/pods/*/http_requests_per_second" | jq .

# ë¶€í•˜ ì¤‘ì§€
kubectl delete pod -n day5-handson load-generator
```

---

## ğŸ› ï¸ Step 2: ê³ ê¸‰ ì•Œë¦¼ ì‹œìŠ¤í…œ (25ë¶„)

### Step 2-1: AlertManager ì„¤ì •

```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'
    
    # ì•Œë¦¼ ë¼ìš°íŒ…
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
      # Critical ì•Œë¦¼ â†’ PagerDuty
      - match:
          severity: critical
        receiver: 'pagerduty'
        continue: true
      # Warning ì•Œë¦¼ â†’ Slack
      - match:
          severity: warning
        receiver: 'slack'
      # Info ì•Œë¦¼ â†’ Email
      - match:
          severity: info
        receiver: 'email'
    
    # ì•Œë¦¼ ìˆ˜ì‹ ì
    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    - name: 'slack'
      slack_configs:
      - channel: '#warnings'
        title: 'âš ï¸ {{ .GroupLabels.alertname }}'
        text: |
          *Severity:* {{ .CommonLabels.severity }}
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
    
    - name: 'pagerduty'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
        description: '{{ .CommonAnnotations.summary }}'
    
    - name: 'email'
      email_configs:
      - to: 'devops@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'YOUR_EMAIL_PASSWORD'
        headers:
          Subject: '[{{ .Status }}] {{ .GroupLabels.alertname }}'
    
    # ì•Œë¦¼ ì–µì œ ê·œì¹™
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'cluster', 'service']
```

```bash
# AlertManager ì„¤ì • ì—…ë°ì´íŠ¸
kubectl apply -f alertmanager-config.yaml

# AlertManager ì¬ì‹œì‘
kubectl rollout restart statefulset alertmanager-prometheus-kube-prometheus-alertmanager -n monitoring
```

### Step 2-2: ì»¤ìŠ¤í…€ ì•Œë¦¼ ê·œì¹™ ìƒì„±

```yaml
# custom-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: application.rules
    interval: 30s
    rules:
    # High HTTP Request Rate
    - alert: HighHTTPRequestRate
      expr: sum(rate(http_requests_total[5m])) by (pod) > 1000
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High HTTP request rate on {{ $labels.pod }}"
        description: "Pod {{ $labels.pod }} is receiving {{ $value }} requests/sec"
    
    # Pod Memory Usage High
    - alert: PodMemoryUsageHigh
      expr: |
        (container_memory_usage_bytes{namespace="day5-handson"} / 
         container_spec_memory_limit_bytes{namespace="day5-handson"}) > 0.9
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod memory usage is above 90%"
        description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"
    
    # Pod CPU Throttling
    - alert: PodCPUThrottling
      expr: |
        rate(container_cpu_cfs_throttled_seconds_total{namespace="day5-handson"}[5m]) > 0.5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod is being CPU throttled"
        description: "Pod {{ $labels.pod }} is being throttled {{ $value | humanizePercentage }} of the time"
    
    # HPA at Max Capacity
    - alert: HPAMaxedOut
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas{namespace="day5-handson"} ==
        kube_horizontalpodautoscaler_spec_max_replicas{namespace="day5-handson"}
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "HPA {{ $labels.horizontalpodautoscaler }} is at maximum capacity"
        description: "HPA has been at max replicas for 15 minutes"
```

```bash
# ì•Œë¦¼ ê·œì¹™ ìƒì„±
kubectl apply -f custom-alerts.yaml

# ì•Œë¦¼ ê·œì¹™ í™•ì¸
kubectl get prometheusrule -n monitoring

# Prometheusì—ì„œ ê·œì¹™ í™•ì¸
# http://localhost:9090/alerts
```

### Step 2-3: ì•Œë¦¼ í…ŒìŠ¤íŠ¸

```bash
# ì˜ë„ì ìœ¼ë¡œ ë†’ì€ ë¶€í•˜ ìƒì„±
for i in {1..10}; do
  kubectl run load-$i --image=busybox --restart=Never -- /bin/sh -c \
    "while true; do wget -q -O- http://metrics-app:8080; done"
done

# AlertManager UI í™•ì¸
kubectl port-forward -n monitoring svc/alertmanager-operated 9093:9093
# http://localhost:9093

# ì•Œë¦¼ ë°œìƒ í™•ì¸
# Slack, Email, PagerDuty í™•ì¸

# ë¶€í•˜ ì¤‘ì§€
for i in {1..10}; do kubectl delete pod load-$i; done
```

---

## ğŸ› ï¸ Step 3: ë©€í‹° í´ëŸ¬ìŠ¤í„° GitOps (25ë¶„)

### Step 3-0: ArgoCD ë¡œê·¸ì¸

**ArgoCD ì„œë²„ ì ‘ì†**:
```bash
# ArgoCD ì„œë²„ í¬íŠ¸í¬ì›Œë”© (ë°±ê·¸ë¼ìš´ë“œ)
kubectl port-forward svc/argocd-server -n argocd 8080:443 > /dev/null 2>&1 &

# ì´ˆê¸° admin ë¹„ë°€ë²ˆí˜¸ í™•ì¸
ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
echo "ArgoCD Password: $ARGOCD_PASSWORD"

# ArgoCD ë¡œê·¸ì¸
argocd login localhost:8080 --username admin --password $ARGOCD_PASSWORD --insecure

# ë¡œê·¸ì¸ í™•ì¸
argocd account get-user-info
```

### Step 3-1: í´ëŸ¬ìŠ¤í„° ë“±ë¡

```bash
# í˜„ì¬ í´ëŸ¬ìŠ¤í„°ë¥¼ ArgoCDì— ë“±ë¡
argocd cluster add $(kubectl config current-context)

# ë“±ë¡ëœ í´ëŸ¬ìŠ¤í„° í™•ì¸
argocd cluster list

# ì¶”ê°€ í´ëŸ¬ìŠ¤í„° ë“±ë¡ (ë‹¤ë¥¸ kubeconfig ì‚¬ìš©)
argocd cluster add dev-cluster --kubeconfig ~/.kube/dev-config
argocd cluster add prod-cluster --kubeconfig ~/.kube/prod-config
```

### Step 3-2: í™˜ê²½ë³„ Application ìƒì„±

```yaml
# apps/dev-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: web-app-dev
  namespace: argocd
spec:
  project: day5-handson
  source:
    repoURL: https://github.com/your-org/your-repo
    targetRevision: develop
    path: helm/web-app
    helm:
      valueFiles:
        - values-dev.yaml
      parameters:
        - name: replicaCount
          value: "2"
        - name: image.tag
          value: "dev-latest"
  destination:
    server: https://dev-cluster-api-server
    namespace: development
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
---
# apps/prod-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: web-app-prod
  namespace: argocd
spec:
  project: day5-handson
  source:
    repoURL: https://github.com/your-org/your-repo
    targetRevision: main
    path: helm/web-app
    helm:
      valueFiles:
        - values-prod.yaml
      parameters:
        - name: replicaCount
          value: "5"
        - name: image.tag
          value: "v1.2.0"
  destination:
    server: https://prod-cluster-api-server
    namespace: production
  syncPolicy:
    automated:
      prune: false  # í”„ë¡œë•ì…˜ì€ ìˆ˜ë™ ì‚­ì œ
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

```bash
# Application ìƒì„±
kubectl apply -f apps/dev-app.yaml
kubectl apply -f apps/prod-app.yaml

# Application ìƒíƒœ í™•ì¸
argocd app list
argocd app get web-app-dev
argocd app get web-app-prod
```

### Step 3-3: App of Apps íŒ¨í„´

```yaml
# root-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: root-app
  namespace: argocd
spec:
  project: day5-handson
  source:
    repoURL: https://github.com/your-org/gitops-repo
    targetRevision: HEAD
    path: apps
  destination:
    server: https://kubernetes.day5-handson.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

**Git Repository êµ¬ì¡°**:
```
gitops-repo/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”œâ”€â”€ monitoring.yaml      # Prometheus, Grafana
â”‚   â”‚   â”œâ”€â”€ logging.yaml         # ELK Stack
â”‚   â”‚   â””â”€â”€ ingress.yaml         # Nginx Ingress
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ app1.yaml
â”‚   â”‚   â””â”€â”€ app2.yaml
â”‚   â””â”€â”€ prod/
â”‚       â”œâ”€â”€ app1.yaml
â”‚       â””â”€â”€ app2.yaml
â””â”€â”€ helm/
    â””â”€â”€ web-app/
        â”œâ”€â”€ Chart.yaml
        â”œâ”€â”€ values.yaml
        â”œâ”€â”€ values-dev.yaml
        â””â”€â”€ values-prod.yaml
```

```bash
# Root App ìƒì„±
kubectl apply -f root-app.yaml

# ëª¨ë“  Application ìë™ ìƒì„± í™•ì¸
argocd app list
```

---

## ğŸ› ï¸ Step 4: í”„ë¡œë•ì…˜ê¸‰ Helm Chart (15ë¶„)

### Step 4-1: Helm Chart ìƒì„±

```bash
# Chart ìƒì„±
helm create production-app

cd production-app/
```

### Step 4-2: Chart.yaml ì‘ì„±

```yaml
# Chart.yaml
apiVersion: v2
name: production-app
description: Production-ready Kubernetes application
type: application
version: 1.0.0
appVersion: "2.1.0"

keywords:
  - web
  - production
  - kubernetes

maintainers:
  - name: DevOps Team
    email: devops@example.com

dependencies:
  - name: postgresql
    version: 12.1.0
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
  - name: redis
    version: 17.3.0
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
```

### Step 4-3: values.yaml ì‘ì„±

```yaml
# values.yaml
replicaCount: 3

image:
  repository: nginx
  pullPolicy: IfNotPresent
  tag: "1.21"

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: app.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: app-tls
      hosts:
        - app.example.com

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# ì˜ì¡´ì„± ì„¤ì •
postgresql:
  enabled: true
  auth:
    username: appuser
    password: changeme
    database: appdb

redis:
  enabled: true
  auth:
    enabled: true
    password: changeme

# ëª¨ë‹ˆí„°ë§
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s

# ë³´ì•ˆ
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
```

### Step 4-4: Chart ê²€ì¦ ë° ë°°í¬

```bash
# Chart ê²€ì¦
helm lint production-app/

# í…œí”Œë¦¿ ë Œë”ë§ í™•ì¸
helm template production-app production-app/

# Dry-run í…ŒìŠ¤íŠ¸
helm install production-app production-app/ --dry-run --debug

# Chart ì„¤ì¹˜
helm install production-app production-app/ \
  --namespace production \
  --create-namespace

# ì„¤ì¹˜ í™•ì¸
helm list -n production
kubectl get all -n production
```

---

## âœ… ì‹¤ìŠµ ì²´í¬í¬ì¸íŠ¸

### âœ… ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ HPA
- [ ] Prometheus Adapter ì •ìƒ ë™ì‘
- [ ] ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸
- [ ] HTTP ìš”ì²­ ìˆ˜ ê¸°ë°˜ HPA ë™ì‘
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œ ìë™ í™•ì¥ í™•ì¸

### âœ… ê³ ê¸‰ ì•Œë¦¼ ì‹œìŠ¤í…œ
- [ ] AlertManager ì„¤ì • ì™„ë£Œ
- [ ] ì»¤ìŠ¤í…€ ì•Œë¦¼ ê·œì¹™ ìƒì„±
- [ ] ê³„ì¸µì  ì•Œë¦¼ ë¼ìš°íŒ… ë™ì‘
- [ ] ì•Œë¦¼ í…ŒìŠ¤íŠ¸ ì„±ê³µ

### âœ… ë©€í‹° í´ëŸ¬ìŠ¤í„° GitOps
- [ ] ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„° ë“±ë¡ ì™„ë£Œ
- [ ] í™˜ê²½ë³„ Application ìƒì„±
- [ ] App of Apps íŒ¨í„´ êµ¬í˜„
- [ ] ìë™ ë™ê¸°í™” í™•ì¸

### âœ… í”„ë¡œë•ì…˜ê¸‰ Helm Chart
- [ ] Chart êµ¬ì¡° ì™„ì„±
- [ ] ì˜ì¡´ì„± ê´€ë¦¬ ì„¤ì •
- [ ] ë³´ì•ˆ ì„¤ì • ì ìš©
- [ ] Chart ë°°í¬ ì„±ê³µ

---

## ğŸš€ ì¶”ê°€ ë„ì „ ê³¼ì œ

### ë„ì „ 1: KEDA ì´ë²¤íŠ¸ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

```yaml
# keda-scaledobject.yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: rabbitmq-consumer
spec:
  scaleTargetRef:
    name: consumer-app
  minReplicaCount: 0
  maxReplicaCount: 30
  triggers:
  - type: rabbitmq
    metadata:
      queueName: orders
      queueLength: "20"
```

### ë„ì „ 2: Canary ë°°í¬

```yaml
# canary-rollout.yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
spec:
  strategy:
    canary:
      steps:
      - setWeight: 20
      - pause: {duration: 5m}
      - setWeight: 50
      - pause: {duration: 5m}
      - setWeight: 100
```

### ë„ì „ 3: ë©€í‹° í…Œë„Œì‹œ ArgoCD

```yaml
# project.yaml
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: team-a
spec:
  destinations:
  - namespace: team-a-*
    server: '*'
  sourceRepos:
  - 'https://github.com/team-a/*'
```

---

## ğŸ§¹ ì‹¤ìŠµ ì •ë¦¬

```bash
# HPA ì‚­ì œ
kubectl delete hpa -n day5-handson metrics-app-hpa

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‚­ì œ
kubectl delete -f metrics-app-deployment.yaml -n day5-handson

# Prometheus Adapter ì‚­ì œ
helm uninstall prometheus-adapter -n monitoring

# Helm Chart ì‚­ì œ (ìˆë‹¤ë©´)
helm uninstall production-app -n production 2>/dev/null || true

# Namespace ì‚­ì œ
kubectl delete namespace day5-handson
kubectl delete namespace production 2>/dev/null || true

# ë˜ëŠ” í´ëŸ¬ìŠ¤í„° ì „ì²´ ì‚­ì œ
kind delete cluster --name challenge-cluster
```

---

## ğŸ’¡ ì‹¤ìŠµ íšŒê³ 

### ğŸ¤ íŒ€ íšŒê³  (10ë¶„)
1. **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­**: "ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ì˜ ì¥ì ì€?"
2. **ê³ ê¸‰ ì•Œë¦¼**: "ê³„ì¸µì  ì•Œë¦¼ ì‹œìŠ¤í…œì´ ì‹¤ë¬´ì—ì„œ ìœ ìš©í• ê¹Œ?"
3. **ë©€í‹° í´ëŸ¬ìŠ¤í„°**: "ì—¬ëŸ¬ í™˜ê²½ì„ í•˜ë‚˜ì˜ ArgoCDë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì˜ ì¥ë‹¨ì ì€?"
4. **Helm ê³ ê¸‰**: "í”„ë¡œë•ì…˜ê¸‰ Chart ì‘ì„± ì‹œ ì£¼ì˜í•  ì ì€?"

### ğŸ“Š í•™ìŠµ ì„±ê³¼
- âœ… ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë” ì •êµí•œ ìŠ¤ì¼€ì¼ë§
- âœ… ë³µì¡í•œ ì•Œë¦¼ ë¼ìš°íŒ…ê³¼ ì–µì œ ê·œì¹™
- âœ… ë©€í‹° í´ëŸ¬ìŠ¤í„° í†µí•© ê´€ë¦¬
- âœ… í”„ë¡œë•ì…˜ê¸‰ Helm Chart ì‘ì„± ëŠ¥ë ¥

### ğŸ¯ ì‹¤ë¬´ ì ìš© ë°©ì•ˆ
- ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ì „ëµ
- íŒ€ë³„ ì•Œë¦¼ ì±„ë„ ë¶„ë¦¬ ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜
- í™˜ê²½ë³„ í´ëŸ¬ìŠ¤í„° ë¶„ë¦¬ ë° í†µí•© ê´€ë¦¬
- í‘œì¤€í™”ëœ Helm Chart í…œí”Œë¦¿ êµ¬ì¶•

---

<div align="center">

**ğŸ¯ ê³ ê¸‰ ìŠ¤ì¼€ì¼ë§** â€¢ **ğŸ”” ì§€ëŠ¥í˜• ì•Œë¦¼** â€¢ **ğŸŒ ë©€í‹° í´ëŸ¬ìŠ¤í„°** â€¢ **ğŸ“¦ í”„ë¡œë•ì…˜ê¸‰ Chart**

*ì‹¤ë¬´ ìš´ì˜ì˜ ëª¨ë“  ê²ƒì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤!*

</div>
