# Session 7: ì»¨í…Œì´ë„ˆ ëª¨ë‹ˆí„°ë§ê³¼ ë¡œê¹…

## ðŸ“ êµê³¼ê³¼ì •ì—ì„œì˜ ìœ„ì¹˜
ì´ ì„¸ì…˜ì€ **Week 2 > Day 3 > Session 7**ë¡œ, ì»¨í…Œì´ë„ˆ ê´€ë¦¬ì™€ ìŠ¤í† ë¦¬ì§€ êµ¬ì„±ì„ ë§ˆì¹œ í›„ ìš´ì˜ í™˜ê²½ì—ì„œ í•„ìˆ˜ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ë¡œê¹… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## í•™ìŠµ ëª©í‘œ (5ë¶„)
- **ì»¨í…Œì´ë„ˆ ëª¨ë‹ˆí„°ë§** ì‹œìŠ¤í…œ êµ¬ì¶• ë° **ë©”íŠ¸ë¦­ ìˆ˜ì§‘**
- **ë¡œê·¸ ë“œë¼ì´ë²„** í™œìš© ë° **ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹…** êµ¬í˜„
- **ì•Œë¦¼ ì‹œìŠ¤í…œ** êµ¬ì„± ë° **ìš´ì˜ ëŒ€ì‹œë³´ë“œ** êµ¬ì¶•

## 1. ì´ë¡ : ì»¨í…Œì´ë„ˆ ê´€ì°°ì„± (Observability) (20ë¶„)

### ê´€ì°°ì„±ì˜ 3ëŒ€ ìš”ì†Œ

```mermaid
graph TB
    subgraph "Observability Pillars"
        A[Metrics] --> D[ì‹œê³„ì—´ ë°ì´í„°]
        B[Logs] --> E[ì´ë²¤íŠ¸ ê¸°ë¡]
        C[Traces] --> F[ìš”ì²­ ì¶”ì ]
    end
    
    subgraph "Docker Monitoring"
        D --> G[CPU, Memory, Network]
        E --> H[Container Logs]
        F --> I[Application Traces]
    end
    
    subgraph "Tools & Solutions"
        G --> J[Prometheus + Grafana]
        H --> K[ELK Stack]
        I --> L[Jaeger, Zipkin]
    end
```

### ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ë¶„ë¥˜

```
ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­:
â”œâ”€â”€ CPU ì‚¬ìš©ë¥  (%)
â”œâ”€â”€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB/GB)
â”œâ”€â”€ ë””ìŠ¤í¬ I/O (IOPS, MB/s)
â”œâ”€â”€ ë„¤íŠ¸ì›Œí¬ I/O (packets/s, MB/s)
â””â”€â”€ íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ìˆ˜

ì»¨í…Œì´ë„ˆ ë©”íŠ¸ë¦­:
â”œâ”€â”€ ì»¨í…Œì´ë„ˆ ìƒíƒœ (running/stopped/failed)
â”œâ”€â”€ ìž¬ì‹œìž‘ íšŸìˆ˜
â”œâ”€â”€ ì´ë¯¸ì§€ í¬ê¸°
â”œâ”€â”€ ë³¼ë¥¨ ì‚¬ìš©ëŸ‰
â””â”€â”€ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìˆ˜

ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­:
â”œâ”€â”€ ì‘ë‹µ ì‹œê°„ (ms)
â”œâ”€â”€ ì²˜ë¦¬ëŸ‰ (requests/s)
â”œâ”€â”€ ì—ëŸ¬ìœ¨ (%)
â”œâ”€â”€ í ê¸¸ì´
â””â”€â”€ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­

ì¸í”„ë¼ ë©”íŠ¸ë¦­:
â”œâ”€â”€ í˜¸ìŠ¤íŠ¸ ë¦¬ì†ŒìŠ¤
â”œâ”€â”€ ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì‹œê°„
â”œâ”€â”€ ìŠ¤í† ë¦¬ì§€ ì„±ëŠ¥
â”œâ”€â”€ ì„œë¹„ìŠ¤ ê°€ìš©ì„±
â””â”€â”€ ë³´ì•ˆ ì´ë²¤íŠ¸
```

### ë¡œê·¸ ë ˆë²¨ ë° êµ¬ì¡°í™”

```
ë¡œê·¸ ë ˆë²¨ ì²´ê³„:

FATAL (0): ì‹œìŠ¤í…œ ì¤‘ë‹¨
â”œâ”€â”€ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜
â”œâ”€â”€ ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”
â””â”€â”€ ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨

ERROR (1): ì˜¤ë¥˜ ë°œìƒ
â”œâ”€â”€ ê¸°ëŠ¥ ë™ìž‘ ì‹¤íŒ¨
â”œâ”€â”€ ë¹ ë¥¸ ëŒ€ì‘ í•„ìš”
â””â”€â”€ ì˜ˆ: API í˜¸ì¶œ ì‹¤íŒ¨

WARN (2): ê²½ê³ 
â”œâ”€â”€ ìž ìž¬ì  ë¬¸ì œ
â”œâ”€â”€ ëª¨ë‹ˆí„°ë§ í•„ìš”
â””â”€â”€ ì˜ˆ: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€

INFO (3): ì •ë³´
â”œâ”€â”€ ì¼ë°˜ì ì¸ ë™ìž‘
â”œâ”€â”€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì¶”ì 
â””â”€â”€ ì˜ˆ: ì‚¬ìš©ìž ë¡œê·¸ì¸

DEBUG (4): ë””ë²„ê·¸
â”œâ”€â”€ ìƒì„¸í•œ ì‹¤í–‰ ì •ë³´
â”œâ”€â”€ ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½
â””â”€â”€ ì˜ˆ: ë³€ìˆ˜ ê°’, í•¨ìˆ˜ í˜¸ì¶œ

êµ¬ì¡°í™”ëœ ë¡œê·¸ í˜•ì‹:
{
  "timestamp": "2024-01-01T12:00:00Z",
  "level": "INFO",
  "service": "web-api",
  "container_id": "abc123",
  "message": "User login successful",
  "user_id": "12345",
  "ip_address": "192.168.1.100",
  "response_time": 150
}
```

## 2. ì‹¤ìŠµ: Docker ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ (15ë¶„)

### ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```bash
# ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d --name web-server nginx:alpine
docker run -d --name database -e MYSQL_ROOT_PASSWORD=secret mysql:8.0
docker run -d --name cache redis:alpine

# ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´
echo "=== Basic Docker Monitoring ==="

# ì‹¤ì‹œê°„ í†µê³„
docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}\t{{.BlockIO}}"

# ê°œë³„ ì»¨í…Œì´ë„ˆ ìƒì„¸ ì •ë³´
docker inspect web-server --format '{{json .State}}' | jq

# ì‹œìŠ¤í…œ ì „ì²´ ì •ë³´
docker system df
docker system events --since "1m" &
EVENTS_PID=$!

sleep 10
kill $EVENTS_PID
```

### ì»¤ìŠ¤í…€ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

```bash
# ê³ ê¸‰ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > container-monitor.sh << 'EOF'
#!/bin/bash

LOG_FILE="/tmp/container-monitor.log"
ALERT_THRESHOLD_CPU=80
ALERT_THRESHOLD_MEM=80

# ë¡œê·¸ í•¨ìˆ˜
log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í•¨ìˆ˜
collect_metrics() {
    local container=$1
    
    # CPU ì‚¬ìš©ë¥  ì¶”ì¶œ
    local cpu_percent=$(docker stats --no-stream --format "{{.CPUPerc}}" $container | sed 's/%//')
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¶”ì¶œ
    local mem_usage=$(docker stats --no-stream --format "{{.MemPerc}}" $container | sed 's/%//')
    
    # ë„¤íŠ¸ì›Œí¬ I/O
    local net_io=$(docker stats --no-stream --format "{{.NetIO}}" $container)
    
    # ë¸”ë¡ I/O
    local block_io=$(docker stats --no-stream --format "{{.BlockIO}}" $container)
    
    # ì»¨í…Œì´ë„ˆ ìƒíƒœ
    local status=$(docker inspect $container --format '{{.State.Status}}')
    
    # ë©”íŠ¸ë¦­ ë¡œê¹…
    log_message "METRICS - Container: $container, CPU: ${cpu_percent}%, Memory: ${mem_usage}%, Status: $status"
    
    # ì•Œë¦¼ ì²´í¬
    if (( $(echo "$cpu_percent > $ALERT_THRESHOLD_CPU" | bc -l) )); then
        log_message "ALERT - High CPU usage in $container: ${cpu_percent}%"
    fi
    
    if (( $(echo "$mem_usage > $ALERT_THRESHOLD_MEM" | bc -l) )); then
        log_message "ALERT - High memory usage in $container: ${mem_usage}%"
    fi
}

# í—¬ìŠ¤ì²´í¬ í•¨ìˆ˜
health_check() {
    local container=$1
    
    # ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ìƒíƒœ í™•ì¸
    if ! docker ps --format "{{.Names}}" | grep -q "^${container}$"; then
        log_message "ALERT - Container $container is not running"
        return 1
    fi
    
    # í”„ë¡œì„¸ìŠ¤ í™•ì¸
    local process_count=$(docker exec $container ps aux | wc -l)
    log_message "HEALTH - Container $container has $process_count processes"
    
    return 0
}

# ë©”ì¸ ëª¨ë‹ˆí„°ë§ ë£¨í”„
log_message "Starting container monitoring..."

for i in {1..10}; do
    echo "=== Monitoring Cycle $i ==="
    
    for container in $(docker ps --format "{{.Names}}"); do
        collect_metrics $container
        health_check $container
    done
    
    echo "Cycle $i completed, sleeping..."
    sleep 5
done

log_message "Monitoring completed"
EOF

chmod +x container-monitor.sh

# ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
./container-monitor.sh

# ë¡œê·¸ í™•ì¸
echo "=== Monitoring Log ==="
tail -20 /tmp/container-monitor.log
```

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„

```bash
# ë¦¬ì†ŒìŠ¤ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
cat > resource-analyzer.sh << 'EOF'
#!/bin/bash

echo "=== Container Resource Analysis ==="

# ì»¨í…Œì´ë„ˆë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘
declare -A cpu_usage
declare -A mem_usage
declare -A net_io

for container in $(docker ps --format "{{.Names}}"); do
    # 5ì´ˆê°„ í‰ê·  ì‚¬ìš©ëŸ‰ ì¸¡ì •
    total_cpu=0
    total_mem=0
    
    for i in {1..5}; do
        cpu=$(docker stats --no-stream --format "{{.CPUPerc}}" $container | sed 's/%//')
        mem=$(docker stats --no-stream --format "{{.MemPerc}}" $container | sed 's/%//')
        
        total_cpu=$(echo "$total_cpu + $cpu" | bc -l)
        total_mem=$(echo "$total_mem + $mem" | bc -l)
        
        sleep 1
    done
    
    avg_cpu=$(echo "scale=2; $total_cpu / 5" | bc -l)
    avg_mem=$(echo "scale=2; $total_mem / 5" | bc -l)
    
    cpu_usage[$container]=$avg_cpu
    mem_usage[$container]=$avg_mem
    
    echo "Container: $container"
    echo "  Average CPU: ${avg_cpu}%"
    echo "  Average Memory: ${avg_mem}%"
    echo ""
done

# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìˆœìœ„
echo "=== Resource Usage Ranking ==="
echo "Top CPU consumers:"
for container in "${!cpu_usage[@]}"; do
    echo "${cpu_usage[$container]} $container"
done | sort -nr | head -3

echo ""
echo "Top Memory consumers:"
for container in "${!mem_usage[@]}"; do
    echo "${mem_usage[$container]} $container"
done | sort -nr | head -3
EOF

chmod +x resource-analyzer.sh
./resource-analyzer.sh
```

## 3. ì‹¤ìŠµ: ë¡œê·¸ ë“œë¼ì´ë²„ ë° ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… (15ë¶„)

### ë¡œê·¸ ë“œë¼ì´ë²„ ì„¤ì •

```bash
# ë‹¤ì–‘í•œ ë¡œê·¸ ë“œë¼ì´ë²„ë¡œ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
echo "=== Log Driver Configuration ==="

# JSON íŒŒì¼ ë¡œê·¸ ë“œë¼ì´ë²„ (ê¸°ë³¸)
docker run -d --name app-json \
    --log-driver json-file \
    --log-opt max-size=10m \
    --log-opt max-file=3 \
    alpine sh -c 'while true; do echo "JSON log: $(date)"; sleep 2; done'

# Syslog ë“œë¼ì´ë²„
docker run -d --name app-syslog \
    --log-driver syslog \
    --log-opt syslog-address=udp://localhost:514 \
    alpine sh -c 'while true; do echo "Syslog: $(date)"; sleep 2; done' || echo "Syslog not available"

# ë¡œê·¸ ì—†ìŒ (ì„±ëŠ¥ ìµœì í™”)
docker run -d --name app-none \
    --log-driver none \
    alpine sh -c 'while true; do echo "No logs: $(date)"; sleep 2; done'

# ë¡œê·¸ í™•ì¸
echo "JSON file logs:"
docker logs app-json | head -5

echo "Syslog logs (if available):"
docker logs app-syslog | head -5 || echo "Syslog logs not accessible via docker logs"

echo "No logs:"
docker logs app-none || echo "No logs available (expected)"
```

### ELK Stack êµ¬ì„±

```bash
# ELK Stack ë„¤íŠ¸ì›Œí¬ ìƒì„±
docker network create elk-network

# Elasticsearch
docker run -d --name elasticsearch \
    --network elk-network \
    -p 9200:9200 \
    -e "discovery.type=single-node" \
    -e "ES_JAVA_OPTS=-Xms512m -Xmx512m" \
    elasticsearch:7.17.0

# Kibana
docker run -d --name kibana \
    --network elk-network \
    -p 5601:5601 \
    -e "ELASTICSEARCH_HOSTS=http://elasticsearch:9200" \
    kibana:7.17.0

# Logstash ì„¤ì • íŒŒì¼ ìƒì„±
mkdir -p elk-config
cat > elk-config/logstash.conf << 'EOF'
input {
  beats {
    port => 5044
  }
  tcp {
    port => 5000
    codec => json
  }
}

filter {
  if [docker] {
    mutate {
      add_field => { "container_name" => "%{[docker][container][name]}" }
    }
  }
  
  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "docker-logs-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
EOF

# Logstash
docker run -d --name logstash \
    --network elk-network \
    -p 5000:5000 \
    -p 5044:5044 \
    -v $(pwd)/elk-config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \
    logstash:7.17.0

# ë¡œê·¸ ìƒì„± ì• í”Œë¦¬ì¼€ì´ì…˜
docker run -d --name log-generator \
    --network elk-network \
    --log-driver json-file \
    alpine sh -c '
        counter=1
        while true; do
            level=$(shuf -n1 -e INFO WARN ERROR DEBUG)
            echo "{\"timestamp\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"level\":\"$level\",\"message\":\"Log entry $counter\",\"service\":\"log-generator\"}"
            counter=$((counter + 1))
            sleep 1
        done
    '

echo "ELK Stack is starting up... (this may take a few minutes)"
echo "Elasticsearch: http://localhost:9200"
echo "Kibana: http://localhost:5601"
```

### Fluentd ë¡œê·¸ ìˆ˜ì§‘

```bash
# Fluentd ì„¤ì • íŒŒì¼
mkdir -p fluentd-config
cat > fluentd-config/fluent.conf << 'EOF'
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

<match docker.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name docker-logs
  type_name _doc
  
  <buffer>
    flush_interval 1s
  </buffer>
</match>

<match **>
  @type stdout
</match>
EOF

# Fluentd ì»¨í…Œì´ë„ˆ
docker run -d --name fluentd \
    --network elk-network \
    -p 24224:24224 \
    -v $(pwd)/fluentd-config:/fluentd/etc \
    fluent/fluentd:v1.14-1

# Fluentd ë¡œê·¸ ë“œë¼ì´ë²„ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
docker run -d --name app-fluentd \
    --log-driver fluentd \
    --log-opt fluentd-address=localhost:24224 \
    --log-opt tag=docker.app \
    alpine sh -c '
        while true; do
            echo "Fluentd log: $(date) - Random number: $RANDOM"
            sleep 3
        done
    '

# ë¡œê·¸ í™•ì¸
sleep 10
docker logs fluentd | tail -10
```

## 4. ì‹¤ìŠµ: Prometheusì™€ Grafana ëª¨ë‹ˆí„°ë§ (10ë¶„)

### Prometheus ì„¤ì •

```bash
# Prometheus ì„¤ì • íŒŒì¼
mkdir -p prometheus-config
cat > prometheus-config/prometheus.yml << 'EOF'
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
  
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
EOF

# ëª¨ë‹ˆí„°ë§ ë„¤íŠ¸ì›Œí¬
docker network create monitoring

# Prometheus
docker run -d --name prometheus \
    --network monitoring \
    -p 9090:9090 \
    -v $(pwd)/prometheus-config/prometheus.yml:/etc/prometheus/prometheus.yml \
    prom/prometheus

# cAdvisor (ì»¨í…Œì´ë„ˆ ë©”íŠ¸ë¦­)
docker run -d --name cadvisor \
    --network monitoring \
    -p 8080:8080 \
    --volume=/:/rootfs:ro \
    --volume=/var/run:/var/run:ro \
    --volume=/sys:/sys:ro \
    --volume=/var/lib/docker/:/var/lib/docker:ro \
    --volume=/dev/disk/:/dev/disk:ro \
    gcr.io/cadvisor/cadvisor:latest

# Node Exporter (ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­)
docker run -d --name node-exporter \
    --network monitoring \
    -p 9100:9100 \
    prom/node-exporter

# Grafana
docker run -d --name grafana \
    --network monitoring \
    -p 3000:3000 \
    -e "GF_SECURITY_ADMIN_PASSWORD=admin" \
    grafana/grafana

echo "Monitoring stack is starting up..."
echo "Prometheus: http://localhost:9090"
echo "Grafana: http://localhost:3000 (admin/admin)"
echo "cAdvisor: http://localhost:8080"
```

### ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```bash
# ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ìƒì„±ê¸°
cat > metrics-generator.py << 'EOF'
#!/usr/bin/env python3
import time
import random
import json
from http.server import HTTPServer, BaseHTTPRequestHandler

class MetricsHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/metrics':
            # Prometheus í˜•ì‹ ë©”íŠ¸ë¦­
            metrics = f"""
# HELP app_requests_total Total number of requests
# TYPE app_requests_total counter
app_requests_total {random.randint(1000, 5000)}

# HELP app_response_time_seconds Response time in seconds
# TYPE app_response_time_seconds histogram
app_response_time_seconds_bucket{{le="0.1"}} {random.randint(10, 50)}
app_response_time_seconds_bucket{{le="0.5"}} {random.randint(50, 100)}
app_response_time_seconds_bucket{{le="1.0"}} {random.randint(100, 200)}
app_response_time_seconds_bucket{{le="+Inf"}} {random.randint(200, 300)}

# HELP app_memory_usage_bytes Memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes {random.randint(50000000, 100000000)}
"""
            self.send_response(200)
            self.send_header('Content-type', 'text/plain')
            self.end_headers()
            self.wfile.write(metrics.encode())
        else:
            self.send_response(404)
            self.end_headers()

if __name__ == '__main__':
    server = HTTPServer(('0.0.0.0', 8000), MetricsHandler)
    print("Metrics server starting on port 8000...")
    server.serve_forever()
EOF

# ë©”íŠ¸ë¦­ ìƒì„±ê¸° ì»¨í…Œì´ë„ˆ
docker run -d --name metrics-app \
    --network monitoring \
    -p 8000:8000 \
    -v $(pwd)/metrics-generator.py:/app/metrics.py \
    python:3.9-alpine sh -c 'cd /app && python metrics.py'

# ë©”íŠ¸ë¦­ í™•ì¸
sleep 5
curl -s http://localhost:8000/metrics | head -10
```

## 5. ì‹¤ìŠµ: ì•Œë¦¼ ë° ëŒ€ì‹œë³´ë“œ êµ¬ì„± (10ë¶„)

### ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì„±

```bash
# Alertmanager ì„¤ì •
mkdir -p alertmanager-config
cat > alertmanager-config/alertmanager.yml << 'EOF'
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@example.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://webhook-receiver:8080/webhook'
EOF

# Prometheus ì•Œë¦¼ ê·œì¹™
cat > prometheus-config/alert-rules.yml << 'EOF'
groups:
- name: container.rules
  rules:
  - alert: HighCPUUsage
    expr: rate(container_cpu_usage_seconds_total[5m]) * 100 > 80
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
      description: "Container {{ $labels.name }} CPU usage is above 80%"
  
  - alert: HighMemoryUsage
    expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100 > 80
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage detected"
      description: "Container {{ $labels.name }} memory usage is above 80%"
EOF

# Alertmanager
docker run -d --name alertmanager \
    --network monitoring \
    -p 9093:9093 \
    -v $(pwd)/alertmanager-config/alertmanager.yml:/etc/alertmanager/alertmanager.yml \
    prom/alertmanager

# ì›¹í›… ìˆ˜ì‹ ê¸° (ì•Œë¦¼ í…ŒìŠ¤íŠ¸ìš©)
cat > webhook-receiver.py << 'EOF'
#!/usr/bin/env python3
import json
from http.server import HTTPServer, BaseHTTPRequestHandler

class WebhookHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        content_length = int(self.headers['Content-Length'])
        post_data = self.rfile.read(content_length)
        
        try:
            alert_data = json.loads(post_data.decode('utf-8'))
            print(f"ALERT RECEIVED: {json.dumps(alert_data, indent=2)}")
        except:
            print(f"RAW ALERT: {post_data.decode('utf-8')}")
        
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b'OK')

if __name__ == '__main__':
    server = HTTPServer(('0.0.0.0', 8080), WebhookHandler)
    print("Webhook receiver starting on port 8080...")
    server.serve_forever()
EOF

docker run -d --name webhook-receiver \
    --network monitoring \
    -v $(pwd)/webhook-receiver.py:/app/webhook.py \
    python:3.9-alpine sh -c 'cd /app && python webhook.py'
```

### ëŒ€ì‹œë³´ë“œ ìžë™í™”

```bash
# Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
cat > setup-dashboard.sh << 'EOF'
#!/bin/bash

# Grafana APIë¥¼ í†µí•œ ëŒ€ì‹œë³´ë“œ ì„¤ì •
GRAFANA_URL="http://localhost:3000"
GRAFANA_USER="admin"
GRAFANA_PASS="admin"

# ë°ì´í„°ì†ŒìŠ¤ ì¶”ê°€
curl -X POST \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Prometheus",
    "type": "prometheus",
    "url": "http://prometheus:9090",
    "access": "proxy",
    "isDefault": true
  }' \
  http://admin:admin@localhost:3000/api/datasources

# ê°„ë‹¨í•œ ëŒ€ì‹œë³´ë“œ ìƒì„±
cat > dashboard.json << 'DASHBOARD_EOF'
{
  "dashboard": {
    "title": "Docker Container Monitoring",
    "panels": [
      {
        "title": "Container CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total[5m]) * 100",
            "legendFormat": "{{ name }}"
          }
        ]
      },
      {
        "title": "Container Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "container_memory_usage_bytes / 1024 / 1024",
            "legendFormat": "{{ name }}"
          }
        ]
      }
    ]
  }
}
DASHBOARD_EOF

# ëŒ€ì‹œë³´ë“œ ì—…ë¡œë“œ
curl -X POST \
  -H "Content-Type: application/json" \
  -d @dashboard.json \
  http://admin:admin@localhost:3000/api/dashboards/db

echo "Dashboard setup completed"
EOF

chmod +x setup-dashboard.sh

# ëŒ€ì‹œë³´ë“œ ì„¤ì • (Grafanaê°€ ì™„ì „ížˆ ì‹œìž‘ëœ í›„)
sleep 30
./setup-dashboard.sh || echo "Dashboard setup will be available once Grafana is fully started"
```

## 6. Q&A ë° ì •ë¦¬ (5ë¶„)

### ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê²€ì¦

```bash
# ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ìƒíƒœ í™•ì¸
echo "=== Monitoring Stack Status ==="

# ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì»¨í…Œì´ë„ˆ ìƒíƒœ
docker ps --filter network=monitoring --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
docker ps --filter network=elk-network --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
echo ""
echo "Service Health Checks:"

# Prometheus
curl -s http://localhost:9090/-/healthy && echo "âœ“ Prometheus is healthy" || echo "âœ— Prometheus is not responding"

# Grafana
curl -s http://localhost:3000/api/health && echo "âœ“ Grafana is healthy" || echo "âœ— Grafana is not responding"

# Elasticsearch
curl -s http://localhost:9200/_cluster/health && echo "âœ“ Elasticsearch is healthy" || echo "âœ— Elasticsearch is not responding"

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸
echo ""
echo "Metrics Collection:"
curl -s http://localhost:9090/api/v1/query?query=up | jq '.data.result | length' && echo "targets are being monitored"

# ë¡œê·¸ ìˆ˜ì§‘ í™•ì¸
echo ""
echo "Log Collection:"
docker logs log-generator | tail -3
docker logs app-fluentd | tail -3

# ìµœì¢… ì •ë¦¬ ê°€ì´ë“œ
cat > monitoring-summary.md << 'EOF'
# Docker Monitoring & Logging Summary

## êµ¬ì¶•ëœ ì‹œìŠ¤í…œ
1. **ë©”íŠ¸ë¦­ ìˆ˜ì§‘**: Prometheus + cAdvisor + Node Exporter
2. **ë¡œê·¸ ìˆ˜ì§‘**: ELK Stack + Fluentd
3. **ì‹œê°í™”**: Grafana ëŒ€ì‹œë³´ë“œ
4. **ì•Œë¦¼**: Alertmanager + Webhook

## ì ‘ì† ì •ë³´
- Prometheus: http://localhost:9090
- Grafana: http://localhost:3000 (admin/admin)
- Kibana: http://localhost:5601
- Elasticsearch: http://localhost:9200
- Alertmanager: http://localhost:9093

## ì£¼ìš” ë©”íŠ¸ë¦­
- CPU ì‚¬ìš©ë¥ : container_cpu_usage_seconds_total
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: container_memory_usage_bytes
- ë„¤íŠ¸ì›Œí¬ I/O: container_network_*
- ë””ìŠ¤í¬ I/O: container_fs_*

## ë¡œê·¸ ë“œë¼ì´ë²„
- json-file: ê¸°ë³¸, ë¡œí…Œì´ì…˜ ì§€ì›
- fluentd: ì¤‘ì•™ ì§‘ì¤‘ì‹ ìˆ˜ì§‘
- syslog: ì‹œìŠ¤í…œ ë¡œê·¸ í†µí•©
- none: ì„±ëŠ¥ ìµœì í™”

## ìš´ì˜ ê¶Œìž¥ì‚¬í•­
1. ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì • í•„ìˆ˜
2. ë©”íŠ¸ë¦­ ë³´ì¡´ ê¸°ê°„ ì •ì±… ìˆ˜ë¦½
3. ì•Œë¦¼ ìž„ê³„ê°’ í™˜ê²½ë³„ ì¡°ì •
4. ëŒ€ì‹œë³´ë“œ ì •ê¸°ì  ì—…ë°ì´íŠ¸
EOF

echo "Monitoring summary created: monitoring-summary.md"
echo ""
echo "âœ“ Monitoring and logging system setup completed!"
```

## ðŸ’¡ í•µì‹¬ í‚¤ì›Œë“œ
- **ê´€ì°°ì„±**: Metrics, Logs, Traces
- **ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ**: Prometheus, Grafana, ELK Stack
- **ë¡œê·¸ ë“œë¼ì´ë²„**: json-file, fluentd, syslog
- **ì•Œë¦¼ ì‹œìŠ¤í…œ**: Alertmanager, ìž„ê³„ê°’, ì›¹í›…

## ðŸ“š ì°¸ê³  ìžë£Œ
- [Docker ë¡œê¹… ë“œë¼ì´ë²„](https://docs.docker.com/config/containers/logging/)
- [Prometheus ëª¨ë‹ˆí„°ë§](https://prometheus.io/docs/)
- [ELK Stack ê°€ì´ë“œ](https://www.elastic.co/guide/)

## ðŸ”§ ì‹¤ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Docker ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ êµ¬í˜„
- [ ] ë¡œê·¸ ë“œë¼ì´ë²„ ì„¤ì • ë° í™œìš©
- [ ] ELK Stack ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹…
- [ ] Prometheus + Grafana ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì„± ë° í…ŒìŠ¤íŠ¸
